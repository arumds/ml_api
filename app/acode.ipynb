{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "acode.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5aGatwMVCPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083bca2c-37d7-4232-bf4c-4b99687e923a"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.0.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=e35f02e4ee82df6734370fbbf11b0c4d82404b14db8e6b1acac8fe8600c5a8e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6VeOu1zWefu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7537335f-5a91-4209-834b-c2ce011e06d1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.metrics import classification_report,confusion_matrix,f1_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "\n",
        "''' Starting preprocessing data '''\n",
        "\n",
        "#Importing data\n",
        "df = pd.read_csv(\"testdata.csv\")\n",
        "\n",
        "###drop id column\n",
        "df = df.drop(['id'], axis=1)\n",
        "\n",
        "###drop duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "##split and remove '0' from the decimal part\n",
        "def strip(x):\n",
        "  return  str(x).split('.')[0]\n",
        "df['amount'] = df['amount'].apply(lambda x: strip(x))\n",
        "df['tax'] = df['tax'].apply(lambda x: strip(x))\n",
        "df['vat_deducation'] = df['vat_deducation'].apply(lambda x: strip(x))\n",
        "\n",
        "# def stripstamp(x):\n",
        "#   return  str(x).split(' ')[0]\n",
        "# df['billdate'] = df['billdate'].apply(lambda x: stripstamp(x))\n",
        "\n",
        "##scale price amount by multiplying with a constant 100\n",
        "def price(x):\n",
        "  return  str(x*100)\n",
        "df['price'] = df['price'].apply(lambda x: price(x))\n",
        "\n",
        "df['billdate'] = pd.to_datetime(df['billdate'])\n",
        "df['billdate_year'] = df['billdate'].dt.year\n",
        "df['billdate_month'] = df['billdate'].dt.month\n",
        "df['billdate_day'] = df['billdate'].dt.day\n",
        "\n",
        "##fill missing values for unit column with null\n",
        "df['unit'] = df[\"unit\"].fillna(\"null\")\n",
        "\n",
        "##fill - product value with noproduct\n",
        "df['product'] = df['product'].replace('-', \"noproduct\", regex=True)\n",
        "\n",
        "##remove special characters in customername and product columns\n",
        "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
        "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
        "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
        "              \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\"]\n",
        "\n",
        "for char in spec_chars:\n",
        "    df['product'] = df['product'].str.replace(char, ' ')\n",
        "    df['customername'] = df['customername'].str.replace(char, ' ')\n",
        "\n",
        "\n",
        "##remove white space and convert to lowecase\n",
        "df['product'] = df['product'].str.replace(' ','').str.lower()\n",
        "df['customername'] = df['customername'].str.replace(' ','').str.lower()\n",
        "\n",
        "###remove invoice records with class frequency =1\n",
        "df_filt = df.groupby('account_code').filter(lambda x: len(x) > 1 )\n",
        "\n",
        "###subset features by dropping label column\n",
        "features = df_filt.drop(['account_code'], axis=1)\n",
        "\n",
        "# # find features with values corresponding to <1% of the total records and drop them\n",
        "#this decreased the accuracy so did not use this approach\n",
        "# counts = features.nunique()\n",
        "# to_del = [i for i,v in enumerate(counts) if (float(v)/features.shape[0]*100) < 1]\n",
        "# feat_to_del = features.columns[to_del]\n",
        "# features = features.drop(feat_to_del,axis=1)\n",
        "\n",
        "###convert non-numeric features to categorical and replace to codes\n",
        "''' start :  encoding and generating pickle files for each feature and saving them'''\n",
        "categorical_columns = features.select_dtypes(include = \"object\").columns\n",
        "for column in categorical_columns:\n",
        "    features[column] = features[column].astype('category')\n",
        "    features[column + '_cat'] = features[column].cat.codes\n",
        "    column_dict = features.set_index(column + '_cat').to_dict()[column]\n",
        "    pickle.dump(column_dict, open(column + \".pickle\", \"wb\"))\n",
        "    features[column] = features[column + '_cat']\n",
        "\n",
        "##drop duplicate columns as cat.codes are replaced into original feature columns\n",
        "features_enc = features.drop(['product_cat', 'amount_cat','billdate', 'price_cat','unit_cat',\n",
        "       'tax_cat', 'invoiceid_cat', 'bodyid_cat', 'invoicestatusid_cat',\n",
        "       'customername_cat', 'currencycode_cat','vat_deducation_cat',\n",
        "       'vat_status_cat'],axis=1)\n",
        "\n",
        "\n",
        "###select target labels column and label encode\n",
        "Y = df_filt['account_code']\n",
        "label_encoder = LabelEncoder()\n",
        "Y = label_encoder.fit_transform(Y)\n",
        "le_name_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "##class to parse json dtypes\n",
        "class NpEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        if isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        # if isinstance(obj, np.ndarray):\n",
        "        #     return obj.tolist()\n",
        "        return super(NpEncoder, self).default(obj)\n",
        "\n",
        "with open('label_mapping.json', 'w') as fp:\n",
        "    json.dump(le_name_mapping,fp, cls=NpEncoder)\n",
        "\n",
        "###Split data into train, test datasets\n",
        "x_train, x_test, y_train, y_test = train_test_split(features_enc, Y, stratify=Y, random_state=7, test_size=0.15)\n",
        "\n",
        "##Invoke classifier\n",
        "xgb_clf = XGBClassifier()\n",
        "\n",
        "''' Fitting the model '''\n",
        "xgb_clf.fit(x_train, y_train)\n",
        "preds = xgb_clf.predict(x_test)\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, preds)\n",
        "\n",
        "print(classification_report(y_test, preds))\n",
        "\n",
        "filename = 'XGBoost.pkl'\n",
        "pickle.dump(xgb_clf, open(filename, 'wb'))\n",
        "\n",
        "xgb_crf = pd.DataFrame(classification_report(y_true = y_test, y_pred = preds, output_dict=True)).transpose()\n",
        "xgb_crf.to_csv('xgb_cr.csv', index = True)\n",
        "\n",
        "\n",
        "#Converting the dataframe into XGBoostâ€™s Dmatrix object\n",
        "dtrain = xgboost.DMatrix(x_train, y_train)\n",
        "\n",
        "''' Tuning the model '''\n",
        "#Bayesian Optimization function for xgboost\n",
        "#specify the parameters  to tune as keyword arguments\n",
        "#Cross validating with the specified parameters in 5 folds and 100 iterations & Return mlogloss\n",
        "def tune_xgb(max_depth,eta):\n",
        "    params = {'max_depth': int(max_depth),\n",
        "              'gamma': 0,\n",
        "              'colsample_bytree' : 1,\n",
        "              'subsample': 0.8,\n",
        "              'eta': eta,\n",
        "              'min_child_weight' : 1,\n",
        "              'objective' : 'multi:softprob',\n",
        "              'num_class' : 151,\n",
        "              'eval_metric': 'mlogloss'}\n",
        "    cv_result = xgboost.cv(params, dtrain, stratified=True, num_boost_round=100, nfold=5, early_stopping_rounds=10)\n",
        "    print(cv_result)\n",
        "    return -1.0 * cv_result['test-mlogloss-mean'].iloc[-1]\n",
        "\n",
        "\n",
        "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
        "xgb_bo = BayesianOptimization(tune_xgb, {'max_depth': (2, 8),\n",
        "                                         'eta':(0.01,0.3)})\n",
        "\n",
        "#performing Bayesian optimization for 20 iterations with 3 steps of random exploration with an #acquisition function of expected improvement\n",
        "xgb_bo.maximize(n_iter=20, init_points=3, acq='ei')\n",
        "\n",
        "#Extracting the best parameters\n",
        "params = xgb_bo.max['params']\n",
        "print(params)\n",
        "#{'eta': 0.15892843654967728, 'max_depth': 6.073989506755048}\n",
        "\n",
        "#Converting the max_depth and n_estimator values from float to int\n",
        "params['max_depth']= int(params['max_depth'])\n",
        "\n",
        "''' Fitting the tuned model '''\n",
        "#Initialize an XGBClassifier with the tuned parameters and fit the training data\n",
        "xgb_tuned = XGBClassifier(**params).fit(x_train, y_train)\n",
        "\n",
        "predic = xgb_tuned.predict(x_test)\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, predic)\n",
        "\n",
        "''' Writing classification report and saving the model as pickle file '''\n",
        "print(classification_report(y_test, predic))\n",
        "xgb_tune_cr = classification_report(y_test, predic)\n",
        "filename1 = 'XGBoost_tuned.pkl'\n",
        "pickle.dump(xgb_tuned, open(filename1, 'wb'))\n",
        "\n",
        "xgb_tune_crf = pd.DataFrame(classification_report(y_true = y_test, y_pred = predic, output_dict=True)).transpose()\n",
        "xgb_tune_crf.to_csv('xgb_tuned_cr.csv', index= True)\n",
        "\n",
        "''' Data pre-processing, data engineering, data modelling and fitting is completed '''\n",
        "\n",
        "\n",
        "# Unused code, can be used to transform with pipeline and featureunion if features have mixed datatypes(non-numeric)\n",
        "# Make use of TextSelecter and NumberSelector to pick a single column\n",
        "# class NumberSelector(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self, key):\n",
        "#         self.key = key\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "#     def transform(self, X):\n",
        "#         return X[[self.key]]\n",
        "#\n",
        "#\n",
        "# product =  Pipeline([('selector', NumberSelector(key='product')),('standard', None)])\n",
        "# amount =  Pipeline([('selector', NumberSelector(key='amount')),('standard', None)])\n",
        "# price =  Pipeline([('selector', NumberSelector(key='price')),('standard', None)])\n",
        "# unit =  Pipeline([('selector', NumberSelector(key='unit')),('standard', None)])\n",
        "# tax =  Pipeline([('selector', NumberSelector(key='tax')),('standard', None)])\n",
        "# invoiceid =  Pipeline([('selector', NumberSelector(key='invoiceid')),('standard', None)])\n",
        "# bodyid =  Pipeline([('selector', NumberSelector(key='bodyid')),('standard', None)])\n",
        "# invoicestatusid =  Pipeline([('selector', NumberSelector(key='invoicestatusid')),('standard', None)])\n",
        "# customername =  Pipeline([('selector', NumberSelector(key='customername')),('standard', None)])\n",
        "# billdate =  Pipeline([('selector', NumberSelector(key='billdate')),('standard', None)])\n",
        "# vat_deducation = Pipeline([('selector', NumberSelector(key='vat_deducation')),('standard', None)])\n",
        "# vat_status =  Pipeline([('selector', NumberSelector(key='vat_status')),('standard', None)])\n",
        "#\n",
        "#\n",
        "# Datafeatures = FeatureUnion([('product', product), ('amount',amount), ('price', price),\n",
        "#                              ('unit', unit), ('tax', tax), ('invoiceid', invoiceid), ('bodyid', bodyid),\n",
        "#                              ('invoicestatusid', invoicestatusid), ('customername', customername),\n",
        "#                              ('billdate', billdate), ('vat_deducation', vat_deducation),\n",
        "#                              ('vat_status', vat_status)])\n",
        "#\n",
        "# pipeline_xgb = Pipeline([\n",
        "#     ('features', Datafeatures),\n",
        "#     ('classifier', XGBClassifier(random_state=1001))])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   gamma   | learni... | max_depth | n_esti... |\n",
            "-------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
            "  UserWarning,\n"
          ]
        }
      ]
    }
  ]
}